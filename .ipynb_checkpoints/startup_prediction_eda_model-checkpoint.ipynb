{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["# Prosperity Prognosticator: Startup Success Prediction\n", "### End-to-End Machine Learning Project"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 1. Import Libraries"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report,\n",
    "    confusion_matrix, roc_auc_score\n",
    ")\n",
    "import joblib\n",
    "\n",
    "print('All libraries imported successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 2. Load Dataset"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "# Download from: https://www.kaggle.com/datasets/manishkc06/startup-success-prediction\n",
    "data = pd.read_csv('startup data.csv')\n",
    "print('Dataset Shape:', data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Columns:', data.columns.tolist())\n",
    "print('\\nData Types:')\n",
    "print(data.dtypes)\n",
    "print('\\nNull Values:')\n",
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 3. Exploratory Data Analysis"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive Statistics\n",
    "print('Descriptive Statistics:')\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["### 3.1 State Distribution"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State distribution\n",
    "data['State'] = 'Other'\n",
    "data.loc[data['state_code'] == 'CA', 'State'] = 'CA'\n",
    "data.loc[data['state_code'] == 'NY', 'State'] = 'NY'\n",
    "data.loc[data['state_code'] == 'MA', 'State'] = 'MA'\n",
    "data.loc[data['state_code'] == 'TX', 'State'] = 'TX'\n",
    "data.loc[data['state_code'] == 'WA', 'State'] = 'WA'\n",
    "\n",
    "state_counts = data['State'].value_counts()\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(state_counts, labels=state_counts.index, autopct='%1.1f%%', startangle=140)\n",
    "plt.title('Distribution of Startups by State')\n",
    "plt.tight_layout()\n",
    "plt.savefig('state_distribution.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["### 3.2 Category Distribution"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category distribution\n",
    "data['category'] = 'Other'\n",
    "top_categories = ['software', 'web', 'mobile', 'enterprise', 'advertising', 'games_video', 'ecommerce', 'biotech', 'consulting', 'messaging']\n",
    "for cat in top_categories:\n",
    "    data.loc[data['category_code'] == cat, 'category'] = cat\n",
    "\n",
    "cat_counts = data['category'].value_counts()\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.pie(cat_counts, labels=cat_counts.index, autopct='%1.1f%%', startangle=140)\n",
    "plt.title('Distribution of Startups by Category')\n",
    "plt.tight_layout()\n",
    "plt.savefig('category_distribution.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["### 3.3 Distribution of Startup Status"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "data['status'].value_counts().plot(kind='bar', color=['steelblue', 'coral'])\n",
    "plt.title('Distribution of Startup Status')\n",
    "plt.xlabel('Status')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig('status_distribution.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["### 3.4 State vs Status"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "state_status = data.groupby(['State', 'status']).size().unstack(fill_value=0)\n",
    "state_status.plot(kind='bar', ax=plt.gca(), color=['steelblue', 'coral'])\n",
    "plt.title('State vs Startup Status')\n",
    "plt.xlabel('State')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=0)\n",
    "plt.legend(title='Status')\n",
    "plt.tight_layout()\n",
    "plt.savefig('state_vs_status.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["### 3.5 Funding Rounds Analysis"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funding_cols = ['has_VC', 'has_angel', 'has_roundA', 'has_roundB', 'has_roundC', 'has_roundD']\n",
    "funding_available = [c for c in funding_cols if c in data.columns]\n",
    "\n",
    "if funding_available:\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(14, 8))\n",
    "    axes = axes.flatten()\n",
    "    for i, col in enumerate(funding_available):\n",
    "        data.groupby([col, 'status']).size().unstack(fill_value=0).plot(\n",
    "            kind='bar', ax=axes[i], color=['steelblue', 'coral'])\n",
    "        axes[i].set_title(col)\n",
    "        axes[i].set_xlabel('')\n",
    "        axes[i].tick_params(axis='x', rotation=0)\n",
    "    plt.suptitle('Funding Rounds vs Startup Status', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('funding_rounds.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["### 3.6 Correlation Heatmap"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_data = data.select_dtypes(include=[np.number])\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.heatmap(numeric_data.corr(), annot=False, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.tight_layout()\n",
    "plt.savefig('correlation_heatmap.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 4. Data Preprocessing"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop irrelevant columns\n",
    "drop_cols = ['Unnamed: 0', 'Unnamed: 6', 'id', 'Unnamed: 0.1', 'name', 'labels',\n",
    "             'founded_at', 'closed_at', 'first_funding_at', 'last_funding_at',\n",
    "             'state_code', 'state_code.1', 'region', 'city', 'zip_code', 'country_code',\n",
    "             'category_code', 'object_id', 'State', 'category']\n",
    "\n",
    "drop_cols_available = [c for c in drop_cols if c in data.columns]\n",
    "df = data.drop(columns=drop_cols_available)\n",
    "\n",
    "# Encode target\n",
    "df['status'] = df['status'].map({'acquired': 1, 'closed': 0})\n",
    "df = df.dropna(subset=['status'])\n",
    "\n",
    "# Fill missing numeric values\n",
    "df = df.fillna(df.median(numeric_only=True))\n",
    "\n",
    "print('Cleaned Dataset Shape:', df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 5. Train-Test Split"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('status', axis=1)\n",
    "y = df['status']\n",
    "\n",
    "# Keep only numeric columns\n",
    "X = X.select_dtypes(include=[np.number])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=116)\n",
    "\n",
    "print('Training set size:', X_train.shape)\n",
    "print('Test set size:', X_test.shape)\n",
    "print('Features used:', X.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 6. Model Building - Random Forest with Hyperparameter Tuning"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline Random Forest\n",
    "rf_base = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_base.fit(X_train, y_train)\n",
    "\n",
    "train_acc_base = accuracy_score(y_train, rf_base.predict(X_train))\n",
    "test_acc_base = accuracy_score(y_test, rf_base.predict(X_test))\n",
    "\n",
    "print(f'Baseline - Train Accuracy: {train_acc_base:.4f}')\n",
    "print(f'Baseline - Test Accuracy:  {test_acc_base:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning with GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    param_grid, cv=5, n_jobs=-1, scoring='accuracy', verbose=1\n",
    ")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print('Best Parameters:', grid_search.best_params_)\n",
    "print('Best CV Score:', grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 7. Evaluating the Best Model"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "train_acc = accuracy_score(y_train, best_model.predict(X_train))\n",
    "test_acc = accuracy_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "print(f'Train Accuracy: {train_acc:.4f}')\n",
    "print(f'Test Accuracy:  {test_acc:.4f}')\n",
    "print(f'ROC AUC Score:  {roc_auc:.4f}')\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(y_test, y_pred, target_names=['Closed', 'Acquired']))\n",
    "\n",
    "results = {\n",
    "    'predictions': y_pred,\n",
    "    'accuracy': test_acc,\n",
    "    'classification_report': classification_report(y_test, y_pred)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Closed', 'Acquired'],\n",
    "            yticklabels=['Closed', 'Acquired'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance\n",
    "feat_imp = pd.Series(best_model.feature_importances_, index=X.columns)\n",
    "feat_imp_top20 = feat_imp.nlargest(20)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "feat_imp_top20.sort_values().plot(kind='barh', color='steelblue')\n",
    "plt.title('Top 20 Feature Importances')\n",
    "plt.xlabel('Importance')\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importance.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy Comparison: Before vs After Hyperparameter Tuning\n",
    "models = ['Baseline RF', 'Tuned RF']\n",
    "train_accs = [train_acc_base, train_acc]\n",
    "test_accs = [test_acc_base, test_acc]\n",
    "\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "bars1 = ax.bar(x - width/2, train_accs, width, label='Train Accuracy', color='steelblue')\n",
    "bars2 = ax.bar(x + width/2, test_accs, width, label='Test Accuracy', color='coral')\n",
    "\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Model Accuracy: Before vs After Hyperparameter Tuning')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(models)\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1.1)\n",
    "for bar in bars1:\n",
    "    ax.annotate(f'{bar.get_height():.2f}', xy=(bar.get_x() + bar.get_width()/2, bar.get_height()),\n",
    "                xytext=(0, 3), textcoords='offset points', ha='center')\n",
    "for bar in bars2:\n",
    "    ax.annotate(f'{bar.get_height():.2f}', xy=(bar.get_x() + bar.get_width()/2, bar.get_height()),\n",
    "                xytext=(0, 3), textcoords='offset points', ha='center')\n",
    "plt.tight_layout()\n",
    "plt.savefig('accuracy_comparison.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## 8. Save the Model"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "joblib.dump(best_model, 'random_forest_model.pkl')\n",
    "print('Model saved as random_forest_model.pkl')\n",
    "\n",
    "# Save feature names for use in Flask app\n",
    "import json\n",
    "with open('feature_names.json', 'w') as f:\n",
    "    json.dump(X.columns.tolist(), f)\n",
    "print('Feature names saved as feature_names.json')\n",
    "\n",
    "# Test loading the model\n",
    "loaded_model = joblib.load('random_forest_model.pkl')\n",
    "test_preds = loaded_model.predict(X_test)\n",
    "print('Model loaded successfully!')\n",
    "print(f'Loaded model test accuracy: {accuracy_score(y_test, test_preds):.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
